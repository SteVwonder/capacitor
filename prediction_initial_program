#!/usr/bin/env python

'''
Initial program built on top of flux-capacitor
Developed with simulating dynamic, hierarchical scheduling in mind
'''

import re
import os
import sys
import imp
import csv
import time
import json
import errno
import Queue
import syslog
import argparse
import subprocess
import multiprocessing as mp

import flux
import flux.kvs as kvs
import flux.jsc as jsc
from flux._core import ffi

local_handle = None
parent_handle = None
sim_handle = None
ALL_MODS = ["sched", "sim_exec", "sim_timer", "init_prog"]

def get_script_dir():
    '''
    Gets the directory of the script
    (specifically the script launched at the command line,
    not necessarily the current module)
    '''
    return os.path.dirname(os.path.realpath(sys.argv[0]))
cap = imp.load_source('flux-capacitor',
                      os.path.join(get_script_dir(), './flux-capacitor'))

def load_sched_module(args):
    ''' Load the sched module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sched',
                'in-sim=true']

    if args.rdl:
        load_cmd.append("rdl-conf={}".format(args.rdl))
    if args.results and False:
        load_cmd.append("resultsfolder={}".format(args.results))
    if args.sched_plugin:
        load_cmd.append("plugin={}".format(args.sched_plugin))

    print "Loading sched module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_sim_module():
    ''' Loads the sim module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sim', 'exit-on-complete=false']
    print "Loading sim module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_exec_module(args):
    ''' Loads the sim_exec module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sim_exec',
                'jobid={}'.format(args.local_jobid)]

    print "Loading exec module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_timer_module(args):
    ''' Loads the sim_timer module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sim_timer',
                'jobid={}'.format(args.local_jobid)]

    print "Loading timer module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_submit_module(args):
    ''' Loads the submit module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'submit',
                'job-csv={}'.format(args.hfile)]

    print "Loading submit module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_modules(args):
    ''' Loads all of the necessary modules into the enclosing flux instance '''

    load_sim_module()
    load_sched_module(args)
    load_exec_module(args)
    load_submit_module(args)

def job_generator(args):
    ''' Reads jobs in from hfile and yields them '''

    with open(args.hfile, 'r') as infile:
        reader = csv.DictReader(infile)
        for line in reader:
            timelimit = int(line['Timelimit'])
            elapsed = int(line['Elapsed'])
            runtime = min(timelimit, elapsed)
            is_hierarchical = int(line['IsH'])
            if is_hierarchical > 0:
                cmd = ['flux', 'start',
                       "{}".format(os.path.realpath(__file__)), line['Hfile']]
                if args.log_dir:
                    cmd.extend(['--log_dir', args.log_dir])
                if args.results:
                    cmd.extend(['-o', args.results])
                if args.sched_plugin:
                    cmd.extend(['--sched_plugin', args.sched_plugin])
                if args.redirect_script:
                    log_path = os.path.join(args.log_dir, "stdout_err-{}".format(args.full_jobid))
                    cmd = [args.redirect_script, log_path] + cmd + ['--redirect_script', args.redirect_script]
                cmd.extend(['child', args.sim_uri, str(args.full_jobid)])
            else:
                cmd = ['sleep', str(runtime)]
            env = cap.get_environment()
            env['FLUX_MODULE_PATH'] = os.environ['FLUX_MODULE_PATH']
            env['FLUX_SCHED_RC_NOOP'] = os.environ['FLUX_SCHED_RC_NOOP']
            new_job = cap.Job(cmd, nnodes=int(line['NNodes']),
                              ntasks=int(line['NCPUS']),
                              runtime=runtime, walltime=timelimit,
                              environ=env,
                              is_hierarchical=is_hierarchical)
            yield new_job

def stream_jobs(args, job_stream):
    '''
    Grabs jobs fromt the generator and puts them into job_stream
    Will complete the job_channel task once we run out of jobs
    '''

    num_jobs = 0

    for job in job_generator(args):
        job_stream.put(job, True)
        num_jobs += 1
    job_stream.put("DONE")
    job_stream.task_done() # for the above "DONE" msg

    return num_jobs

def xterm():
    ''' Launches xterm '''
    output = subprocess.check_output(["xterm"])
    if len(output) > 0:
        print output

def start_sim():
    ''' Sends a Flux RPC to start the simulation '''
    local_handle.rpc_create("sim.starttoken")

def join_sim(args):
    ''' Sends a Flux RPC to join the simulation '''
    rank = local_handle.get_rank()
    local_uri = ffi.string(local_handle.flux_attr_get("local-uri", None))
    payload = {
        'mod_name' : 'init_prog.{}'.format(args.full_jobid),
        'next_event' : -1,
        'rank' : rank,
        'uri' : local_uri
    }
    sim_handle.rpc_create("sim.join", payload=json.dumps(payload))

def send_childbirth_msg(prog_args):
    '''
    Send an RPC to the parent instances letting it know that the child
    (this instance) has completed its setup
    '''
    print "Sending childbirth message"
    local_uri = ffi.string(local_handle.flux_attr_get("local-uri", None))
    payload_str = json.dumps({'child-uri' : local_uri, "jobid" : prog_args.local_jobid})
    parent_handle.event_send("sched.childbirth", payload=payload_str)

def send_jobssubmitted_msg(num_jobs):
    '''
    Send an RPC to the scheduler that contains the number of jobs submitted
    '''
    payload_str = json.dumps({"count": num_jobs})
    return local_handle.rpc_create("sched.alljobssubmitted", payload=payload_str)

def send_simleave_msgs(args):
    ''' Sends a Flux RPC to leave the simulation '''
    print "Sending sim.leave messages"
    for mod_prefix in ALL_MODS:
        mod_name = "{}.{}".format(mod_prefix, args.full_jobid)
        payload_str = json.dumps({'mod_name' : mod_name})
        sim_handle.rpc_create("sim.leave", payload=payload_str)

def send_death_msg(args):
    ''' Sends a flux RPC to notify the parent scheduler of this instance's "death" '''
    payload_str = json.dumps({'jobid' : args.local_jobid})
    print "Sending sched.death message, payload: {}".format(payload_str)
    parent_handle.rpc_create("sched.death", payload=payload_str)

def send_reply_msg(args, message):
    ''' Send Flux RPC in response to the sim's trigger '''

    payload = message.payload

    for mod_prefix in ALL_MODS:
        mod_name = "{}.{}".format(mod_prefix, args.full_jobid)
        del payload['event_timers'][mod_name] #= -1

    payload['mod_name'] = "init_prog.{}".format(args.full_jobid)
    sim_handle.rpc_create('sim.reply', payload=json.dumps(payload))

def trigger_cb(handle, typemask, message, args):
    '''
    Callback that is run when triggered by the simulation
    Cleans up by removing all of the modules from this instance from the simulation
    Also sends a death message to the parent instance
    '''
    print "Initial program {} was triggered".format(args.full_jobid)
    time.sleep(1)
    send_simleave_msgs(args)
    # TODO: fix racecondition
    time.sleep(1)
    # TODO: remove timers from reply
    send_reply_msg(args, message)
    handle.reactor_stop(handle.get_reactor())

def event_reactor_proc(in_stream, out_stream, state_stream):
    fh = flux.Flux()
    def jsc_cb(jcb_str, arg, errnum):
        jcb = json.loads(jcb_str)
        nstate = jcb[jsc.JSC_STATE_PAIR][jsc.JSC_STATE_PAIR_NSTATE]
        jobid = int(jcb['jobid'])
        value = jsc.job_num2state(nstate)
        state_stream.put((jobid, value))
    def stop_cb(fh, watcher, fd_int, revents, stream):
        print "reactor proc received msg on stream: {}".format(stream.get())
        fh.reactor_stop(fh.get_reactor())
    print "Registering callback with JSC"
    jsc.notify_status(fh, jsc_cb, state_stream)
    with flux.core.watchers.FDWatcher(fh, in_stream._reader.fileno(), flux.FLUX_POLLIN,
                        stop_cb, in_stream) as fd_watch:
        out_stream.put("JSC_DONE")
        print "Starting event reactor"
        if fh.reactor_run(fh.get_reactor(), 0) < 0:
            fh.fatal_error("event_reactor_proc", "reactor run failed")
    print "Event reactor exited"

def scheduler_proc(state_stream, sched):
    ret = 0
    while (len(sched.pending_jobs) + len(sched.running_jobs) > 0) or len(sched.completed_jobs) == 0:
        while True:
            try:
                (key, value) = state_stream.get(True, 3)
                sched.state_change(key, value)
            except Queue.Empty:
                break
        print "Pending jobs: {}, running jobs: {}, completed jobs: {}".format(len(sched.pending_jobs), len(sched.running_jobs), len(sched.completed_jobs))

    if len(sched.pending_jobs) + len(sched.running_jobs) == 0:
        print "All jobs complete, exiting scheduler_proc"
    else:
        print "Some jobs did not finish"

class SimScheduler(object):
    '''
    Scheduler made to run under the simulator.
    Inherits from FluxScheduler, mainly overrides the check_feasibility method
    '''

    def __init__(self, interface=None):
        self.jobs = set()
        self.pending_jobs = set()
        self.running_jobs = set()
        self.completed_jobs = set()
        self.interface = interface

    def state_change(self, job_id, state):
        print "Job {} changed to {}".format(job_id, state)
        if self.interface is not None:
            self.interface.state_changed(job_id, state)
        if state == 'submitted':
            self.jobs.add(job_id)
            self.pending_jobs.add(job_id)
        if state == 'running':
            self.pending_jobs.remove(job_id)
            self.running_jobs.add(job_id)
        if state == 'complete':
            self.running_jobs.remove(job_id)
            self.completed_jobs.add(job_id)
        return 0


class SimInterface(cap.UserInterface):
    '''
    Interact with the flux scheduler that is in sim mode
    Add extra information to the KVS upon submission
    Log job information upon completion
    '''

    fieldnames = ['id', 'nnodes', 'ntasks', 'starting_time', 'complete_time',
                  'walltime', 'is_hierarchical']

    def __init__(self, flux_handle, outstream):
        print "Initializing SimInterface"
        self.fh = flux_handle
        self.writer_stream = outstream
        self.writer = csv.DictWriter(outstream, self.fieldnames)
        self.writer.writeheader()
        self.stream = sys.stdout

    def submitted(self, job):
        '''
        Runs after a job is submitted but before it is scheduled
        '''
        pass

    def state_changed(self, job_id, state):
        '''
        Runs after a job changes state
        When a job completes, write out relavant job info to a file
        '''
        def try_key_exists(fh, key):
            key_exists = kvs.exists(fh, key)
            return key_exists

        def try_get_dir(fh, key):
            kvs_dir = kvs.get_dir(fh, key)
            return kvs_dir

        if state == 'complete':
            jobdir_key = 'lwj.{}'.format(job_id)
            complete_key = '{}.complete_time'.format(jobdir_key)
            print "Looking for kvs entry {}, since job {} completed".format(complete_key, job_id)
            while not try_key_exists(self.fh, complete_key):
                print "{} kvs entry not found, waiting for it to be created".format(complete_key)
                time.sleep(1)
            kvsdir = try_get_dir(self.fh, jobdir_key)
            # TODO: figure out why listing a kvsdir is not returning everything
            rowdict = {}
            for key in self.fieldnames:
                try:
                    rowdict[key] = kvsdir[key]
                except KeyError:
                    pass
            rowdict['id'] = job_id
            self.writer.writerow(rowdict)

class Tee(object):
    '''
    Allows for printing to a file and flux's dmesg buffer simultaneously
    Modeled after the Unix 'tee' comand
    '''
    def __init__(self, name, mode, buffering=None, flux_handle=None):
        self.file = open(name, mode, buffering=buffering)
        if buffering:
            self.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering)
        else:
            self.stdout = sys.stdout
        self.flux_handle = flux_handle
        sys.stdout = self
    def __del__(self):
        sys.stdout = self.stdout
        self.file.close()
    def write(self, data):
        self.file.write(data)
        if self.flux_handle:
            new_data = data.strip()
            if len(new_data) > 0:
                self.flux_handle.log(syslog.LOG_DEBUG, new_data)
    def flush(self):
        self.file.flush()

def setup_logging(args, flux_handle=None):
    '''
    Replace sys.stdout with an instance of Tee
    Also set the enclosing broker to write out its logs to a file
    '''
    if args.log_dir:
        filename = os.path.join(args.log_dir, "{}-initprog.out".format(args.full_jobid))
        Tee(filename, 'w', buffering=0, flux_handle=flux_handle)
        if flux_handle:
            flux_handle.log_set_appname("init_prog")
    else:
        sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)

    if args.log_dir:
        log_filename = os.path.join(args.log_dir, "{}-broker.out".format(args.full_jobid))
        setattr_cmd = ['flux', 'setattr', 'log-filename', log_filename]
        subprocess.check_output(setattr_cmd)

def parse_command_line():
    ''' Parses and validates command line arguments '''

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument('hfile',
                            help="job file")
    arg_parser.add_argument('--results', '-o',
                            help="directory to store the results in")
    arg_parser.add_argument('--xterm', action='store_true',
                            help='launch an interactive xterm')
    arg_parser.add_argument('--log_dir', help="log stdout to a file (arg = directory)")
    arg_parser.add_argument('--sched_plugin', help="which sched plugin to use")
    arg_parser.add_argument('--redirect_script', help="path to the bash script to redirect stdout/stderr")
    arg_parser.add_argument('--rdl', '-r',
                             help="path to the rdl file")

    args = arg_parser.parse_args()

    args.local_jobid = 0
    args.full_jobid = 0
    args.sim_uri = os.environ['FLUX_URI']

    # Command line argument validation
    assert os.path.isfile(args.hfile)

    if args.results:
        assert os.path.isdir(args.results)
    if args.rdl:
        assert os.path.isfile(args.rdl)

    return args


def main():
    ''' Starts all 3 processes: reactor, sched, and job  '''

    args = parse_command_line()

    global local_handle
    global sim_handle
    global parent_handle
    local_handle = flux.Flux()
    sim_handle = local_handle

    setup_logging(args, local_handle)

    print "Loading modules"
    load_modules(args)

    state_stream = mp.Queue(10000)
    reactor_in_stream = mp.Queue(1)
    reactor_out_stream = mp.Queue(1)
    job_stream = mp.JoinableQueue()

    if args.xterm:
        xterm_proc = mp.Process(target=xterm)
        xterm_proc.start()

    outfilename = os.path.join(args.results, "job-{}".format(args.full_jobid))
    with open(outfilename, 'wb', 0) as outfile:
        interface = SimInterface(local_handle, outfile)
        sched = SimScheduler(interface=interface)

        # Process to process to job events
        reactor_proc = mp.Process(target=event_reactor_proc,
                                  args=(reactor_in_stream, reactor_out_stream, state_stream))
        reactor_proc.start()

        #TODO: unnecessary?
        time.sleep(1)

        # Process to run Flux-Capacitor's scheduler
        sched_proc = mp.Process(target=scheduler_proc,
                                args=(state_stream, sched))
        sched_proc.start()

        #sched_rpc_req = send_jobssubmitted_msg(num_jobs)
        #join_sim(args)

        reactor_out_stream.get() # wait for jsc to be registered
        time.sleep(1)
        print "Starting simulation"
        start_sim()

        # trigger_topic = "init_prog.{}.trigger".format(args.full_jobid)
        # print "Registering watcher for '{}' requests".format(trigger_topic)
        # local_handle.event_subscribe(trigger_topic)
        # with local_handle.msg_watcher_create(trigger_cb,
        #                                      type_mask=flux.FLUX_MSGTYPE_EVENT,
        #                                      topic_glob=trigger_topic,
        #                                      args=args) as msg_watch:
        #     print "Entering reactor"
        #     if local_handle.reactor_run(local_handle.get_reactor(), 0) < 0:
        #         print "reactor start failed"
        #         local_handle.fatal_error("init_prog.main", "reactor start failed!")

        # print "Exited reactor"
        # Wait for all of the processes to finish

        sched_proc.join()
        print "Sending DONE to reactor proc via reactor stream"
        reactor_in_stream.put("DONE")
        reactor_proc.join()

    if args.xterm:
        xterm_proc.join()
    print "All subprocesses shutdown"
    print "initial program is done and exiting"

if __name__ == "__main__":
    main()


